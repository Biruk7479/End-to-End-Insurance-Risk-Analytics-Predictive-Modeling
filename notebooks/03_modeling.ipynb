{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import DataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "from ml_models import ClaimSeverityModel, PremiumOptimizationModel, build_claim_severity_model\n",
    "from shap_analysis import SHAPAnalyzer, perform_shap_analysis\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414aa977",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DataLoader()\n",
    "df = loader.load_data('../data/MachineLearningRating_v3.txt', sep='|')\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = preprocessor.handle_missing_values(df, strategy='median')\n",
    "\n",
    "# Convert data types\n",
    "df_clean = preprocessor.convert_data_types(df_clean)\n",
    "\n",
    "# Create features\n",
    "df_clean = preprocessor.create_features(df_clean)\n",
    "\n",
    "print(f\"Preprocessed data: {df_clean.shape}\")\n",
    "print(f\"\\nNew features created: LossRatio, ProfitMargin, HasClaim, VehicleAge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution of TotalClaims\n",
    "axes[0].hist(df_clean['TotalClaims'], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Distribution of TotalClaims', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('TotalClaims')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Log-transformed distribution\n",
    "df_with_claims = df_clean[df_clean['TotalClaims'] > 0]\n",
    "axes[1].hist(np.log1p(df_with_claims['TotalClaims']), bins=50, edgecolor='black', color='orange')\n",
    "axes[1].set_title('Log-transformed TotalClaims (Claims > 0)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('log(TotalClaims + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal policies: {len(df_clean):,}\")\n",
    "print(f\"Policies with claims: {len(df_with_claims):,} ({len(df_with_claims)/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"Policies without claims: {len(df_clean) - len(df_with_claims):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305ecee",
   "metadata": {},
   "source": [
    "## 2. Build Claim Severity Models\n",
    "\n",
    "We'll predict TotalClaims for policies that have claims (TotalClaims > 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for modeling\n",
    "numerical_features = [\n",
    "    'SumInsured', 'CalculatedPremiumPerTerm', 'Kilowatts',\n",
    "    'Cubiccapacity', 'VehicleAge', 'RegistrationYear'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Province', 'VehicleType', 'Make', 'CoverType',\n",
    "    'Gender', 'MaritalStatus'\n",
    "]\n",
    "\n",
    "# Filter existing columns\n",
    "numerical_features = [col for col in numerical_features if col in df_with_claims.columns]\n",
    "categorical_features = [col for col in categorical_features if col in df_with_claims.columns]\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "claim_model = ClaimSeverityModel()\n",
    "\n",
    "# Prepare features\n",
    "X, y = claim_model.prepare_features(\n",
    "    df_with_claims,\n",
    "    target_col='TotalClaims',\n",
    "    categorical_cols=categorical_features,\n",
    "    numerical_cols=numerical_features\n",
    ")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfba878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = claim_model.train_test_split_data(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76429955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_train_scaled, X_test_scaled = claim_model.scale_features(X_train, X_test)\n",
    "\n",
    "print(\"Features scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31811e0b",
   "metadata": {},
   "source": [
    "### 2.1 Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "models = claim_model.train_all_models(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nTrained models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711c609",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = claim_model.evaluate_all_models(X_test_scaled, y_test)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R2', 'MSE']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    results_df[metric].plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(results_df[metric]):\n",
    "        ax.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest model by RMSE: {results_df.index[0]} (RMSE: {results_df.iloc[0]['RMSE']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6b9e6",
   "metadata": {},
   "source": [
    "### 2.3 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance_rf = claim_model.get_feature_importance('random_forest', top_n=10)\n",
    "\n",
    "print(\"\\nTop 10 Features (Random Forest):\")\n",
    "print(feature_importance_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d12cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance_rf['feature'], feature_importance_rf['importance'], color='teal', edgecolor='black')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 10 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance_rf.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from XGBoost\n",
    "feature_importance_xgb = claim_model.get_feature_importance('xgboost', top_n=10)\n",
    "\n",
    "print(\"\\nTop 10 Features (XGBoost):\")\n",
    "print(feature_importance_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eee7cf",
   "metadata": {},
   "source": [
    "### 2.4 Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual TotalClaims', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted TotalClaims', fontsize=12)\n",
    "axes[0].set_title(f'Predictions vs Actual ({best_model_name})', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted TotalClaims', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f319d",
   "metadata": {},
   "source": [
    "## 3. SHAP Analysis for Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a54045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SHAP analysis on best model\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# Convert to DataFrame for SHAP\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=claim_model.feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=claim_model.feature_names)\n",
    "\n",
    "print(\"Initialized SHAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP analyzer\n",
    "shap_analyzer = SHAPAnalyzer(best_model, model_type='tree')\n",
    "\n",
    "# Create explainer\n",
    "shap_analyzer.create_explainer(X_train_df)\n",
    "\n",
    "# Calculate SHAP values (use subset for speed)\n",
    "X_test_sample = X_test_df.sample(min(1000, len(X_test_df)), random_state=42)\n",
    "shap_values = shap_analyzer.calculate_shap_values(X_test_sample)\n",
    "\n",
    "print(f\"SHAP values calculated for {len(X_test_sample)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "shap_analyzer.plot_summary(X_test_sample, max_display=10,\n",
    "                          save_path='../reports/figures/shap_summary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot\n",
    "shap_analyzer.plot_bar(X_test_sample, max_display=10,\n",
    "                      save_path='../reports/figures/shap_bar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168412c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from SHAP\n",
    "shap_feature_importance = shap_analyzer.get_feature_importance_df(X_test_sample, top_n=10)\n",
    "\n",
    "print(\"\\nTop 10 Features by SHAP:\")\n",
    "print(shap_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba68be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate interpretation report\n",
    "interpretation_report = shap_analyzer.generate_interpretation_report(X_test_sample, top_n=10)\n",
    "\n",
    "print(interpretation_report)\n",
    "\n",
    "# Save report\n",
    "with open('../reports/shap_interpretation.md', 'w') as f:\n",
    "    f.write(interpretation_report)\n",
    "\n",
    "print(\"\\nSHAP interpretation report saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a867670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for single prediction\n",
    "shap_analyzer.plot_waterfall(X_test_sample, sample_index=0,\n",
    "                            save_path='../reports/figures/shap_waterfall_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef549d4",
   "metadata": {},
   "source": [
    "## 4. Premium Optimization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize premium optimization model\n",
    "premium_model = PremiumOptimizationModel()\n",
    "\n",
    "# Prepare features for premium prediction\n",
    "X_premium, y_premium = premium_model.prepare_features(\n",
    "    df_clean,\n",
    "    target_col='CalculatedPremiumPerTerm',\n",
    "    categorical_cols=categorical_features,\n",
    "    numerical_cols=numerical_features\n",
    ")\n",
    "\n",
    "print(f\"Premium model features: {X_premium.shape}\")\n",
    "print(f\"Premium model target: {y_premium.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = premium_model.train_test_split_data(X_premium, y_premium)\n",
    "X_train_p_scaled, X_test_p_scaled = premium_model.scale_features(X_train_p, X_test_p)\n",
    "\n",
    "print(f\"Premium training set: {X_train_p_scaled.shape}\")\n",
    "print(f\"Premium test set: {X_test_p_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train claim probability model\n",
    "claim_prob_model = premium_model.train_claim_probability_model(X_train_p_scaled, y_train_p)\n",
    "\n",
    "# Train premium prediction models\n",
    "premium_models = premium_model.train_premium_models(X_train_p_scaled, y_train_p)\n",
    "\n",
    "print(f\"\\nTrained premium models: {list(premium_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22540b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate premium models\n",
    "premium_results = {}\n",
    "\n",
    "for model_name, model in premium_models.items():\n",
    "    y_pred_p = model.predict(X_test_p_scaled)\n",
    "    metrics = premium_model.evaluate_regression(y_test_p, y_pred_p)\n",
    "    premium_results[model_name] = metrics\n",
    "    print(f\"{model_name} - RMSE: {metrics['RMSE']:.2f}, R2: {metrics['R2']:.4f}\")\n",
    "\n",
    "# Create comparison\n",
    "premium_results_df = pd.DataFrame(premium_results).T\n",
    "premium_results_df = premium_results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\nPremium Model Performance:\")\n",
    "print(premium_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907e5a5",
   "metadata": {},
   "source": [
    "## 5. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296589ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save best claim severity model\n",
    "import joblib\n",
    "joblib.dump(best_model, f'../models/claim_severity_{best_model_name}.pkl')\n",
    "joblib.dump(claim_model.scaler, '../models/claim_severity_scaler.pkl')\n",
    "joblib.dump(claim_model.label_encoders, '../models/claim_severity_encoders.pkl')\n",
    "\n",
    "# Save premium model\n",
    "best_premium_model_name = premium_results_df.index[0]\n",
    "best_premium_model = premium_models[best_premium_model_name]\n",
    "joblib.dump(best_premium_model, f'../models/premium_{best_premium_model_name}.pkl')\n",
    "joblib.dump(premium_model.scaler, '../models/premium_scaler.pkl')\n",
    "joblib.dump(premium_model.label_encoders, '../models/premium_encoders.pkl')\n",
    "\n",
    "print(\"Models saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384858d8",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Insights\n",
    "\n",
    "### Model Performance\n",
    "- **Best Claim Severity Model**: Shows model performance metrics\n",
    "- **Best Premium Model**: Shows premium prediction performance\n",
    "\n",
    "### Key Findings from SHAP Analysis\n",
    "1. Most important features for claim prediction identified\n",
    "2. Feature interactions and their impact quantified\n",
    "3. Model decisions made interpretable for business stakeholders\n",
    "\n",
    "### Business Recommendations\n",
    "1. Use models to optimize premium pricing based on risk factors\n",
    "2. Focus on top features identified by SHAP for risk assessment\n",
    "3. Implement dynamic pricing based on customer segments\n",
    "4. Monitor model performance and retrain periodically"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
